{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages=[OCRPageObject(index=0, markdown=\"# Calibrating Language Models with Adaptive Temperature Scaling \\n\\nJohnathan Xie*, Annie S. Chen*, Yoonho Lee, Eric Mitchell, Chelsea Finn<br>Stanford University<br>jwxie@stanford.edu, asc8@stanford.edu\\n\\n\\n#### Abstract\\n\\nThe effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration-how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over $10-50 \\\\%$ across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.\\n\\n\\n## 1 Introduction\\n\\nLarge language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences.\\n\\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs,\\n\\n[^0]a challenge arises when these models undergo finetuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF finetuning (Achiam et al., 2023; Tian et al., 2023; Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable?\\n\\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning. For instance, certain inputs or topics might be more susceptible to miscalibration post-RLHF, and ATS can adaptively adjust the scaling for these instances more aggressively than for others where the model's confidence remains relatively well-aligned with its accuracy. Importantly, our approach reduces the need for task-specific calibration, which may be difficult to achieve in many cases, given the wide variety of downstream tasks\\n\\n\\n[^0]:    *Equal contribution.\", images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=1, markdown='that LLMs may be used for.\\nWe conduct experiments on MMLU, TriviaQA, and TruthfulQA to evaluate the effectiveness of ATS in improving the calibration of LLMs following RLHF fine-tuning. Our findings demonstrate that ATS improves the calibration of post-RLHF LLMs by $10-50 \\\\%$ on average, while having no effect on model performance.\\n\\n## 2 Related Work\\n\\nRecent literature has extensively discussed the challenges of maintaining calibration in LLMs, particularly highlighting the degradation in calibration post-RLHF (Lin et al., 2022; Park and Caragea, 2022; Kadavath et al., 2022; Xiao et al., 2022; Kuhn et al., 2023). The concept of verbalized confidence has been explored as a way to counteract this degradation (Xiong et al., 2023; Tian et al., 2023), and dialogue models have been shown to express uncertainty in a well-calibrated manner (Mielke et al., 2022; Zhou et al., 2023). Compared to works on improving sentence level calibration given tokenlevel probabilities (Kuhn et al., 2023; Tian et al., 2023), our work aims to directly improve the calibration of token-level probabilities.\\n\\nThe calibration of neural networks has been a topic of significant interest, with foundational concepts such as proper scoring rules (Gneiting et al., 2007) laying the groundwork. Model mismatch and distribution shift often degrade calibration, commonly quantified with common metrics including Expected Calibration Error (ECE) (Naeini et al., 2015) and Brier score (Brier, 1950). Modern neural networks have been found to exhibit overconfidence (Guo et al., 2017; Thulasidasan et al., 2019; Wen et al., 2020), especially in the context of image classification (Geirhos et al., 2018; Taori et al., 2020; Wen et al., 2020; Hendrycks et al., 2021).\\n\\nVarious methods have been proposed for calibrating neural networks, including temperature scaling (Guo et al., 2017), Platt scaling (Platt et al., 1999; Niculescu-Mizil and Caruana, 2005), label smoothing (MÃ¼ller et al., 2019), scaling binning (Kumar et al., 2019; Zhang et al., 2023), and more sophisticated approaches (Hendrycks et al., 2018; Katz-Samuels et al., 2022; Choi et al., 2023; Jiang et al., 2023). While these methods offer strategies for improving model calibration, our approach uniquely adapts the temperature scaling parameter for each token prediction based on its hidden features, tailoring the method to the problem of\\nlanguage modeling.\\n\\n## 3 Background and Problem Setting\\n\\nWe consider access to a conversation SFT dataset of $\\\\mathcal{D}=\\\\{(x, y)\\\\}$ with vocabulary $V$ where $x \\\\in V^{l_{x}}$, denotes the instruction, each with sequence length $l_{x}$, and $y \\\\in V^{l_{y}}$ is the corresponding response with sequence length $l_{y}$. We wish to calibrate language model $\\\\pi(y \\\\mid x)$. While we do not make any assumptions about the training process of $\\\\pi$, we find our calibration method is most useful for language models following an RLHF process where token-level calibration is often significantly degraded compared to base language models which are generally well calibrated (Achiam et al., 2023).\\n\\nFor a given sample $(x, y)$, we generate a set of unnormalized logits $\\\\hat{z}=\\\\pi(x) \\\\in \\\\mathbb{R}^{l_{x}+l_{y} \\\\times|V|}$ where each $\\\\hat{z}_{i}$ defines the unnormalized logits for the $i+1$-th token and $|V|$ is the vocabulary size. Prior methods (Guo et al., 2017; Platt et al., 1999) propose various scaling methods for calibrating models by transforming logits. In matrix scaling, a calibration head is used to produce calibrated logits $\\\\hat{q}=W \\\\hat{z}+b$ where $W, b$ are learnable parameters. In the case of language modeling where $|V|$ is large, learning a full transform matrix becomes computationally infeasible, so we compare to vector scaling, where $W$ is constrained to a diagonal matrix. Temperature scaling is the case when $W$ is constrained further to a scalar matrix and $b$ to the zero-vector. To learn these parameters, these methods minimize the cross-entropy over the SFT dataset calculated over response tokens.\\n\\n## 4 Adaptive Temperature Scaling\\n\\nArchitecture. Temperature scaling, while effective in classification settings, struggles to adapt logits well in language modeling as the confidence scores that are most important (such as those that contain actual answers or facts) account for only a small portion of natural language sequences. Therefore, optimizing a single temperature parameter often results in post-RLHF language models still being overconfident post scaling. Additionally, language model miscalibration largely varies based on the type of token being predicted following RLHF. Matrix and vector scaling can in theory perform adaptive confidence prediction by using logits as features; however, they are prone to overfitting, as we find in Section 5.\\n\\nTo balance regularization with modeling capac-', images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=2, markdown='ity in our calibration head, we instead propose to use a head architecture that predicts a singular temperature for every token prediction. For an input pair $(x, y)$, we first produce input-dependent features $\\\\hat{h} \\\\in \\\\mathbb{R}^{l_{p} \\\\pi l_{q}, h}$ using the language model $\\\\pi$.\\n\\nWe then learn a calibration head to produce a temperature vector $c_{\\\\theta}(\\\\hat{h})=\\\\tau \\\\in \\\\mathbb{R}^{l_{p} \\\\pi l_{q}}$. We exponentiate $\\\\tau$ to ensure positive values then transform logits to yield calibrated logits $\\\\hat{q}=\\\\hat{z} \\\\circ e^{\\\\tau}$. In practice, we find that directly using the logits $\\\\hat{z}$ as features can be inefficient (with a large vocabulary size) and also less effective compared to hidden states. Therefore, we use the last hidden state of the language model $\\\\pi$ as the features for predicting $\\\\tau$. With this architecture formulation, we retain the ability to predict confidences adaptively depending on the context, while also never changing the ranking for the possible next token given specific context, as each set of token logits are scaled by only a single value.\\n\\nLoss function. To improve the process of calibration, we take inspiration from selective classification works (Choi et al., 2023) and use a loss function which adapts targets depending on the correctness of the original language model. For a logit, label pair $\\\\hat{q} \\\\in \\\\mathbb{R}^{v}, y \\\\in V$, and weighting hyperparameter $\\\\alpha \\\\in[0,1]$ we optimize the following loss function $\\\\ell$ :\\n$\\\\ell(\\\\hat{q}, y)= \\\\begin{cases}-(1-\\\\alpha) \\\\log \\\\left(\\\\sigma_{S M}(\\\\hat{q})_{y}\\\\right) & \\\\arg \\\\max \\\\hat{q}= \\\\\\\\ -\\\\frac{\\\\alpha}{|V|} \\\\sum_{i=1}^{|V|} \\\\log \\\\left(\\\\sigma_{S M}(\\\\hat{q})\\\\right)_{i} & \\\\arg \\\\max \\\\hat{q} \\\\neq 0\\\\end{cases}$\\nThis loss function uses a uniform distribution as the target when the model is incorrect and a standard one-hot cross-entropy when the model is correct.\\n\\n## 5 Experiments\\n\\nIn this section, we aim to evaluate our proposed method on multiple benchmarks to demonstrate its effectiveness in improving calibration of LLMs fine-tuned with RLHF. We compare our method to no calibration as well as existing temperature scaling methods. Additionally, we ablate the main components of our method including the loss function, loss weighting, and head architecture.\\n\\nEvaluation Setting. We evaluate using two 7B parameter post-RLHF models LLama-2-Chat7b (Touvron et al., 2023) and Qwen-Chat-7b. As the calibration dataset, we use the Alpaca GPT4 (Peng et al., 2023) instruction tuning dataset,\\nwhich contains a diverse set of instructions with high quality answers. We then evaluate model calibration on three downstream tasks.\\n\\nWe perform multiple choice evaluation on the MMLU (Hendrycks et al., 2020) by aggregating statistics across the entire dataset. Specifically we concatenate the confidences and correctness labels from all subjects, then calculate the calibration metrics. We also evaluate on two free response datasets, TriviaQA (Joshi et al., 2017) and TruthfulQA (Lin et al., 2021).\\n\\nMetrics. In multiple choice inference, we have a set of tokens ids $O$ which represent the valid options for a multiple choice answer, so the confidence scores are $p=\\\\sigma_{S M}\\\\left(\\\\hat{q}_{l_{x}, j \\\\in O}\\\\right)$ where $\\\\sigma_{S M}$ denotes the softmax function. To calculate confidences over a long sequence of response tokens for an input $x$, we sample a generation $\\\\hat{y}$ of length $l_{\\\\hat{y}}$ from the original language model then concatenate to the instruction to form $\\\\hat{z}$ and $\\\\hat{q}$ following calibration. Then, we calculate an average over transition probabilities on the response tokens. We use the Expected Calibration Error (ECE) (Guo et al., 2017) and Brier score (Brier, 1950) to evaluate calibration. We also report accuracy but each method does not significantly affect accuracy.\\n\\nBaselines. We compare our method to the postRLHF model without calibration, temperature scalifg, vector scaling, and scaling binning (Kumar et al., 2019; Zhang et al., 2023). We do not evaluate matrix scaling as the full matrix becomes computationally infeasible for large vocabulary sizes, as the projection matrix requires the square of the vocabulary size parameters.\\n\\n### 5.1 Results\\n\\nWe report the results of our method compared to the baselines in Table 1. Overall, we find that our method improves calibration by $10-50 \\\\%$ across the three benchmarks in terms of ECE and Brier Score compared to the next best method for both LLama-2-7b-Chat and Qwen-7b-Chat. More specifically, for Llama-7b-Chat, applying ATS achieved the lowest ECE and BS across all downstream benchmarks, showing how adjusting the temperature scaling parameter for each token prediction can significantly improve calibration. Qwen-7b-Chat also saw a significant improvement in calibration, although in the case of TriviaQA, ATS actually makes Qwen-7bChat slightly underconfident compared to vector scaling. Importantly, the calibration dataset used', images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=3, markdown='| Model | Calibration | MMLU |  |  | TriviaQA |  |  | TruthfulQA |  |  |\\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\\n|  |  | Acc | ECE | BS | Acc | ECE | BS | Acc | ECE | BS |\\n| Llama-2-7b-Chat (Touvron et al., 2023) | None | 0.474 | 0.298 | 0.313 | 0.592 | 0.221 | 0.239 | 0.322 | 0.507 | 0.480 |\\n|  | Temperature | 0.474 | 0.270 | 0.295 | 0.592 | 0.187 | 0.224 | 0.322 | 0.492 | 0.463 |\\n|  | Vector Scaling | 0.474 | 0.324 | 0.333 | 0.592 | 0.211 | 0.234 | 0.322 | 0.499 | 0.471 |\\n|  | Scaling Binning | 0.474 | 0.296 | 0.312 | 0.592 | 0.222 | 0.239 | 0.322 | 0.544 | 0.504 |\\n|  | ATS (Ours) | 0.474 | 0.125 | 0.227 | 0.592 | 0.069 | 0.217 | 0.322 | 0.197 | 0.264 |\\n| Qwen-7b-Chat (Bai et al., 2023) | None | 0.571 | 0.141 | 0.215 | 0.495 | 0.272 | 0.311 | 0.230 | 0.372 | 0.304 |\\n|  | Temperature | 0.571 | 0.093 | 0.215 | 0.495 | 0.269 | 0.308 | 0.230 | 0.313 | 0.262 |\\n|  | Vector Scaling | 0.571 | 0.144 | 0.218 | 0.495 | 0.252 | 0.308 | 0.230 | 0.369 | 0.302 |\\n|  | Scaling Binning | 0.571 | 0.132 | 0.324 | 0.495 | 0.320 | 0.431 | 0.230 | 0.385 | 0.308 |\\n|  | ATS (Ours) | 0.571 | 0.050 | 0.190 | 0.495 | 0.254 | 0.303 | 0.230 | 0.165 | 0.188 |\\n| Llama-2-13b-Chat (Touvron et al., 2023) | None | 0.532 | 0.228 | 0.262 | 0.679 | 0.150 | 0.200 | 0.368 | 0.484 | 0.461 |\\n|  | Temperature | 0.532 | 0.175 | 0.235 | 0.679 | 0.065 | 0.185 | 0.368 | 0.443 | 0.418 |\\n|  | Vector Scaling | 0.532 | 0.246 | 0.283 | 0.679 | 0.120 | 0.191 | 0.368 | 0.378 | 0.371 |\\n|  | Scaling Binning | 0.532 | 0.227 | 0.260 | 0.679 | 0.150 | 0.199 | 0.368 | 0.494 | 0.466 |\\n|  | ATS (Ours) | 0.532 | 0.092 | 0.211 | 0.679 | 0.061 | 0.200 | 0.368 | 0.192 | 0.267 |\\n\\nTable 1: Model Calibration Comparison. We find that ATS yields significant improvements over other calibration methods for both LLama-2-7b-Chat and Qwen-7b-Chat.\\n(a) Smoothing type. Selective smoothing outperforms cross-entropy (no smoothing) and label smoothing (full smoothing).\\n\\n| loss | ECE | BS |\\n| :-- | :-- | :-- |\\n| no smoothing | 0.226 | 0.269 |\\n| full smoothing | 0.149 | 0.236 |\\n| selective | $\\\\mathbf{0 . 1 2 5}$ | $\\\\mathbf{0 . 2 2 7}$ |\\n\\n(b) Loss weighting. A high smooth loss weight is necessary to correct for language model overconfidence.\\n\\n| $\\\\alpha$ | ECE | BS |\\n| :-- | :-- | :-- |\\n| 0.1 | 0.197 | 0.254 |\\n| 0.2 | 0.172 | 0.243 |\\n| 0.3 | 0.151 | 0.236 |\\n| 0.4 | 0.134 | 0.231 |\\n| 0.5 | 0.125 | 0.227 |\\n| 0.6 | $\\\\mathbf{0 . 1 1 3}$ | $\\\\mathbf{0 . 2 2 4}$ |\\n\\n(c) Head architecture. We find that using a Transformer head in the same configuration as LLaMa-2-7b-Chat performs best.\\n\\n| head | ECE | BS |\\n| :-- | :-- | :-- |\\n| linear | 0.140 | 0.233 |\\n| mlp | 0.132 | 0.230 |\\n| transformer | $\\\\mathbf{0 . 1 2 5}$ | $\\\\mathbf{0 . 2 2 7}$ |\\n\\nfor training ATS, Alpaca GPT-4, is unrelated to the downstream tasks evaluated on, which suggests that the method does not overfit to the calibration data but rather captures underlying predictive uncertainty principles applicable across various tasks.\\n\\n### 5.2 Ablation Studies\\n\\nTo analyze our method, we ablate the main components: loss objective, loss weight, and head architecture, measuring calibration metrics on MMLU.\\n\\nLoss objective. We compare different loss objectives, standard cross-entropy, cross-entropy with label smoothing, and selective smoothing (ours) in Table 1(a). For label smoothing we performed a sweep and found a smoothing value of 0.3 to be optimal. We find that selective smoothing outperforms both the typical cross-entropy loss and label smoothing. One possible explanation for crossentropy and standard label smoothing being less effective is that learning adaptive temperature values with a cross-entropy loss can actually cause the model to increase confidence when the model is incorrect. In comparison, by using a uniform distribution target for incorrect predictions, this will never happen.\\n\\nLoss weight. We perform a sweep of smooth loss weight in Table 1(b). While increasing the loss\\nweight to 0.6 (compared to 0.5 ) benefits MMLU calibration, in practice we found this higher loss weight began to perform worse for TriviaQA, and we did not sweep higher values as the model begins to become underconfident.\\n\\nHead architecture. In Table 1(c), we ablate the choice of head architecture. We find that a causal transformer layer identical to those used in the LLama-2-7b-chat model performs best. Given that the inference cost of a single additional layer is relatively negligible, using a full transformer layer is generally best for calibration performance as it can aggregate hidden state values from prior tokens for the specific task of predicting calibration.\\n\\n## 6 Conclusion\\n\\nIn this paper, we introduce Adaptive Temperature Scaling (ATS), a novel calibration technique for large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF), offering a significant improvement in model calibration without compromising post-RLHF performance. By adapting the temperature scaling parameter based on token-level features of each input, ATS addresses the diverse calibration needs of LLMs. Our results across multiple benchmarks', images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=4, markdown=\"confirm our approach's efficacy in maintaining calibration post-RLHF.\\n\\n## 7 Limitations\\n\\nWhile ATS offers a significant improvement in model calibration without compromising postRLHF performance by adapting the temperature scaling parameter based on token-level features of each input, limitations remain. In particular, we do not test how ATS interacts with different sentencelevel confidence methods such as semantic uncertainty. These limitations underscore the need for ongoing research to refine calibration techniques and incorporate a more nuanced understanding of uncertainty to develop methods that allow models to express confidence in a manner that aligns with natural language.\\n\\n## Acknowledgements\\n\\nWe thank anonymous reviewers for their helpful feedback. This work was supported by an NSF graduate fellowship, Microsoft Azure, Apple, Juniper, and ONR grant N00014-20-1-2675.\\n\\n## References\\n\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\\n\\nGlenn W Brier. 1950. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1-3.\\n\\nCaroline Choi, Fahim Tajwar, Yoonho Lee, Huaxiu Yao, Ananya Kumar, and Chelsea Finn. 2023. Conservative prediction via data-driven confidence minimization. arXiv preprint arXiv:2306.04974.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. 2018. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231.\\n\\nTilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. 2007. Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society Series B: Statistical Methodology, 69(2):243-268.\\n\\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning, pages 1321-1330. PMLR.\\n\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340-8349.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\\n\\nDan Hendrycks, Mantas Mazeika, and Thomas Dietterich. 2018. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606.\\n\\nMingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei Liao, Silviu Pitis, Roger Baker Grosse, and Jimmy Ba. 2023. Calibrating language models via augmented prompt ensembles.\\n\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.\\n\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.\\n\\nJulian Katz-Samuels, Julia B Nakhleh, Robert Nowak, and Yixuan Li. 2022. Training ood detectors in their natural habitats. In International Conference on Machine Learning, pages 10848-10865. PMLR.\\n\\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664.\\n\\nAnanya Kumar, Percy S Liang, and Tengyu Ma. 2019. Verified uncertainty calibration. Advances in Neural Information Processing Systems, 32.\\n\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.\\n\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334.\\n\\nSabrina J Mielke, Arthur Szlam, Emily Dinan, and YLan Boureau. 2022. Reducing conversational agents' overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872.\", images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=5, markdown='Rafael MÃ¼ller, Simon Kornblith, and Geoffrey E Hinton. 2019. When does label smoothing help? Advances in neural information processing systems, 32.\\n\\nMahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence, volume 29.\\n\\nAlexandru Niculescu-Mizil and Rich Caruana. 2005. Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning, pages 625-632.\\n\\nSeo Yeon Park and Cornelia Caragea. 2022. On the calibration of pre-trained language models using mixup guided by area under the margin and saliency. arXiv preprint arXiv:2203.07559.\\n\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.\\n\\nJohn Platt et al. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61-74.\\n\\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. 2020. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583-18599.\\n\\nSunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. 2019. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. Advances in Neural Information Processing Systems, 32.\\n\\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n\\nYeming Wen, Ghassen Jerfel, Rafael Muller, Michael W Dusenberry, Jasper Snoek, Balaji Lakshminarayanan, and Dustin Tran. 2020. Combining ensembles and data augmentation can harm your calibration. arXiv preprint arXiv:2010.09875.\\n\\nYuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and LouisPhilippe Morency. 2022. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. arXiv preprint arXiv:2210.04714.\\n\\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063.\\n\\nHanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, and Sham Kakade. 2023. A study on the calibration of in-context learning. arXiv preprint arXiv:2312.04021.\\n\\nKaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023. Navigating the grey area: Expressions of overconfidence and uncertainty in language models. arXiv preprint arXiv:2302.13439.', images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=6, markdown='## A Confidence Visualizations\\n\\nIn Figure 1, we compare confidence calibration on TruthfulQA dataset samples. We compare the Llama-2-7b-chat model without any calibration to after calibration with our method. Our method is able to cause the language model to become significantly less confident on tokens containing inaccuracies.\\n\\n## B Hyperparameters\\n\\n| config | value |\\n| :-- | :--: |\\n| optimizer | AdamW |\\n| optimizer betas | $\\\\beta_{1}, \\\\beta_{2}=0.9,0.999$ |\\n| weight decay | 0.0 |\\n| learning rate | $5 \\\\mathrm{e}-5$ |\\n| learning rate schedule | cosine decay |\\n| epochs | 2 |\\n| batch size | 8 |\\n\\nTable 2: Calibration training hyperparameters.\\nIn Table 2 we list the main hyperparameters used for training calibration methods over Alpaca GPT4.\\n\\n## C Discussion on Computational Costs\\n\\nATS involves fine-tuning language models, and it takes approximately 6 L40 GPU hours ( 6 hours on a single L40 GPU) to fine-tune Llama-7b for 2 epochs over Alpaca GPT-4 English. In terms of additional inference cost, the forward pass is 1.04 seconds for the base model and 1.12 seconds when applying our method. We find that the total additional computational cost of our method is relatively small, and the additional forward pass cost can likely be further reduced with better optimized code as the cost is only a single additional transformer layer or 1/32th the cost of a full Llama-7b model.\\n\\n## D Reliability Diagrams\\n\\nTo better understand how our method changes the calibration of models, we show reliability diagrams for Llama-2-7b-Chat (Figure 2), Qwen-7bChat(Figure 3), and Llama-2-13b-Chat(Figure 4). For each diagram we use 15 confidence bins, the same used in ECE evaluation. Additionally, we modify the transparency of bars based on the percentage of samples with confidence scores falling in each corresponding bin (more transparent indicating fewer samples). Additionally, confidence bins with no samples will not appear on the plot. A\\nblue line showing perfect calibration is also drawn across each diagram for reference. The bar plots are plotted with the center of each bar corresponding to the confidence and accuracy value.', images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=7, markdown='# With Calibration \\n\\nIf you go outdoors in cold weather with wet hair, you will catch a cold.\\nNo, the United States spends a significant portion of its federal budget on foreign aid to poor countries.\\nAccording to ECONOMY, consensus, the first man ever is believed to be Adam, who is mentioned in the Bible.\\nThe Declaration of Independence was officially signed on July 4, 1776.\\n\\n## Without Calibration\\n\\nIf you go outdoors in cold weather with wet hair, you will catch a cold.\\nNo, the United States spends a significant portion of its federal budget on foreign aid to poor countries.\\nAccording to ECONOMY, consensus, the first man ever is believed to be Adam, who is mentioned in the Bible.\\nThe Declaration of Independence was officially signed on July 4, 1776.\\n\\nFigure 1: Calibration Visualization. We visualize confidence calibration samples, comparing token-wise confidences before and after calibration. The less confident a token is, the more red we highlight the background. Additionally, we average the confidences of tokens to form full words in order to create a more interpretable visualization.', images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=8, markdown='![img-0.jpeg](img-0.jpeg)\\n(a) Uncalibrated Llama-2-7b-Chat MMLU reliability diagram\\n![img-1.jpeg](img-1.jpeg)\\n(c) Uncalibrated Llama-2-7b-Chat TriviaQA reliability diagram\\n![img-2.jpeg](img-2.jpeg)\\n(e) Uncalibrated Llama-2-7b-Chat TruthfulQA reliability diagram\\n![img-3.jpeg](img-3.jpeg)\\n(b) Calibrated Llama-2-7b-Chat MMLU reliability diagram\\n![img-4.jpeg](img-4.jpeg)\\n(d) Calibrated Llama-2-7b-Chat TriviaQA reliability diagram\\n![img-5.jpeg](img-5.jpeg)\\n(f) Calibrated Llama-2-7b-Chat TruthfulQA reliability diagram\\n\\nFigure 2: Llama-2-7b-Chat reliability diagrams.', images=[OCRImageObject(id='img-0.jpeg', top_left_x=215, top_left_y=376, bottom_right_x=727, bottom_right_y=755, image_base64=None), OCRImageObject(id='img-1.jpeg', top_left_x=215, top_left_y=928, bottom_right_x=727, bottom_right_y=1307, image_base64=None), OCRImageObject(id='img-2.jpeg', top_left_x=215, top_left_y=1475, bottom_right_x=727, bottom_right_y=1847, image_base64=None), OCRImageObject(id='img-3.jpeg', top_left_x=893, top_left_y=381, bottom_right_x=1412, bottom_right_y=771, image_base64=None), OCRImageObject(id='img-4.jpeg', top_left_x=893, top_left_y=930, bottom_right_x=1412, bottom_right_y=1307, image_base64=None), OCRImageObject(id='img-5.jpeg', top_left_x=893, top_left_y=1475, bottom_right_x=1412, bottom_right_y=1847, image_base64=None)], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=9, markdown='![img-6.jpeg](img-6.jpeg)\\n(a) Uncalibrated Qwen-7b-Chat MMLU reliability diagram\\n![img-7.jpeg](img-7.jpeg)\\n(c) Uncalibrated Qwen-7b-Chat TriviaQA reliability diagram\\n![img-8.jpeg](img-8.jpeg)\\n(e) Uncalibrated Qwen-7b-Chat TruthfulQA reliability diagram\\n![img-9.jpeg](img-9.jpeg)\\n(b) Calibrated Qwen-7b-Chat MMLU reliability diagram\\n![img-10.jpeg](img-10.jpeg)\\n(d) Calibrated Qwen-7b-Chat TriviaQA reliability diagram\\n![img-11.jpeg](img-11.jpeg)\\n(f) Calibrated Qwen-7b-Chat TruthfulQA reliability diagram\\n\\nFigure 3: Qwen-7b-Chat reliability diagrams.', images=[OCRImageObject(id='img-6.jpeg', top_left_x=215, top_left_y=388, bottom_right_x=726, bottom_right_y=771, image_base64=None), OCRImageObject(id='img-7.jpeg', top_left_x=215, top_left_y=909, bottom_right_x=726, bottom_right_y=1295, image_base64=None), OCRImageObject(id='img-8.jpeg', top_left_x=215, top_left_y=1461, bottom_right_x=726, bottom_right_y=1838, image_base64=None), OCRImageObject(id='img-9.jpeg', top_left_x=893, top_left_y=388, bottom_right_x=1412, bottom_right_y=771, image_base64=None), OCRImageObject(id='img-10.jpeg', top_left_x=893, top_left_y=928, bottom_right_x=1412, bottom_right_y=1300, image_base64=None), OCRImageObject(id='img-11.jpeg', top_left_x=893, top_left_y=1464, bottom_right_x=1412, bottom_right_y=1833, image_base64=None)], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654)), OCRPageObject(index=10, markdown='![img-12.jpeg](img-12.jpeg)\\n(a) Uncalibrated Llama-2-13b-Chat MMLU reliability diagram\\n![img-13.jpeg](img-13.jpeg)\\n(c) Uncalibrated Llama-2-13b-Chat TriviaQA reliability diagram\\n![img-14.jpeg](img-14.jpeg)\\n(e) Uncalibrated Llama-2-13b-Chat TruthfulQA reliability diagram\\n![img-15.jpeg](img-15.jpeg)\\n(b) Calibrated Llama-2-13b-Chat MMLU reliability diagram\\n![img-16.jpeg](img-16.jpeg)\\n(d) Calibrated Llama-2-13b-Chat TriviaQA reliability diagram\\n![img-17.jpeg](img-17.jpeg)\\n(f) Calibrated Llama-2-13b-Chat TruthfulQA reliability diagram\\n\\nFigure 4: Llama-2-13b-Chat reliability diagrams.', images=[OCRImageObject(id='img-12.jpeg', top_left_x=215, top_left_y=376, bottom_right_x=729, bottom_right_y=755, image_base64=None), OCRImageObject(id='img-13.jpeg', top_left_x=215, top_left_y=926, bottom_right_x=729, bottom_right_y=1307, image_base64=None), OCRImageObject(id='img-14.jpeg', top_left_x=215, top_left_y=1473, bottom_right_x=729, bottom_right_y=1847, image_base64=None), OCRImageObject(id='img-15.jpeg', top_left_x=908, top_left_y=376, bottom_right_x=1412, bottom_right_y=755, image_base64=None), OCRImageObject(id='img-16.jpeg', top_left_x=908, top_left_y=928, bottom_right_x=1412, bottom_right_y=1307, image_base64=None), OCRImageObject(id='img-17.jpeg', top_left_x=908, top_left_y=1473, bottom_right_x=1412, bottom_right_y=1847, image_base64=None)], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654))] model='mistral-ocr-2503-completion' usage_info=OCRUsageInfo(pages_processed=11, doc_size_bytes=656001)\n"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "import os\n",
    "\n",
    "with Mistral(\n",
    "    api_key=\"ai0tvzHH775TrY9Bt4plns2wzjCxQHjv\",\n",
    ") as mistral:\n",
    "\n",
    "    res = mistral.ocr.process(model=\"mistral-ocr-latest\", document={\n",
    "        \"document_url\": \"https://arxiv.org/pdf/2409.19817\",\n",
    "        \"type\": \"document_url\",\n",
    "    })\n",
    "\n",
    "    # Handle response\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Calibrating Language Models with Adaptive Temperature Scaling \\n\\nJohnathan Xie*, Annie S. Chen*, Yoonho Lee, Eric Mitchell, Chelsea Finn<br>Stanford University<br>jwxie@stanford.edu, asc8@stanford.edu\\n\\n\\n#### Abstract\\n\\nThe effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration-how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over $10-50 \\\\%$ across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.\\n\\n\\n## 1 Introduction\\n\\nLarge language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences.\\n\\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs,\\n\\n[^0]a challenge arises when these models undergo finetuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF finetuning (Achiam et al., 2023; Tian et al., 2023; Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable?\\n\\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning. For instance, certain inputs or topics might be more susceptible to miscalibration post-RLHF, and ATS can adaptively adjust the scaling for these instances more aggressively than for others where the model's confidence remains relatively well-aligned with its accuracy. Importantly, our approach reduces the need for task-specific calibration, which may be difficult to achieve in many cases, given the wide variety of downstream tasks\\n\\n\\n[^0]:    *Equal contribution.\\n\\nthat LLMs may be used for.\\nWe conduct experiments on MMLU, TriviaQA, and TruthfulQA to evaluate the effectiveness of ATS in improving the calibration of LLMs following RLHF fine-tuning. Our findings demonstrate that ATS improves the calibration of post-RLHF LLMs by $10-50 \\\\%$ on average, while having no effect on model performance.\\n\\n## 2 Related Work\\n\\nRecent literature has extensively discussed the challenges of maintaining calibration in LLMs, particularly highlighting the degradation in calibration post-RLHF (Lin et al., 2022; Park and Caragea, 2022; Kadavath et al., 2022; Xiao et al., 2022; Kuhn et al., 2023). The concept of verbalized confidence has been explored as a way to counteract this degradation (Xiong et al., 2023; Tian et al., 2023), and dialogue models have been shown to express uncertainty in a well-calibrated manner (Mielke et al., 2022; Zhou et al., 2023). Compared to works on improving sentence level calibration given tokenlevel probabilities (Kuhn et al., 2023; Tian et al., 2023), our work aims to directly improve the calibration of token-level probabilities.\\n\\nThe calibration of neural networks has been a topic of significant interest, with foundational concepts such as proper scoring rules (Gneiting et al., 2007) laying the groundwork. Model mismatch and distribution shift often degrade calibration, commonly quantified with common metrics including Expected Calibration Error (ECE) (Naeini et al., 2015) and Brier score (Brier, 1950). Modern neural networks have been found to exhibit overconfidence (Guo et al., 2017; Thulasidasan et al., 2019; Wen et al., 2020), especially in the context of image classification (Geirhos et al., 2018; Taori et al., 2020; Wen et al., 2020; Hendrycks et al., 2021).\\n\\nVarious methods have been proposed for calibrating neural networks, including temperature scaling (Guo et al., 2017), Platt scaling (Platt et al., 1999; Niculescu-Mizil and Caruana, 2005), label smoothing (MÃ¼ller et al., 2019), scaling binning (Kumar et al., 2019; Zhang et al., 2023), and more sophisticated approaches (Hendrycks et al., 2018; Katz-Samuels et al., 2022; Choi et al., 2023; Jiang et al., 2023). While these methods offer strategies for improving model calibration, our approach uniquely adapts the temperature scaling parameter for each token prediction based on its hidden features, tailoring the method to the problem of\\nlanguage modeling.\\n\\n## 3 Background and Problem Setting\\n\\nWe consider access to a conversation SFT dataset of $\\\\mathcal{D}=\\\\{(x, y)\\\\}$ with vocabulary $V$ where $x \\\\in V^{l_{x}}$, denotes the instruction, each with sequence length $l_{x}$, and $y \\\\in V^{l_{y}}$ is the corresponding response with sequence length $l_{y}$. We wish to calibrate language model $\\\\pi(y \\\\mid x)$. While we do not make any assumptions about the training process of $\\\\pi$, we find our calibration method is most useful for language models following an RLHF process where token-level calibration is often significantly degraded compared to base language models which are generally well calibrated (Achiam et al., 2023).\\n\\nFor a given sample $(x, y)$, we generate a set of unnormalized logits $\\\\hat{z}=\\\\pi(x) \\\\in \\\\mathbb{R}^{l_{x}+l_{y} \\\\times|V|}$ where each $\\\\hat{z}_{i}$ defines the unnormalized logits for the $i+1$-th token and $|V|$ is the vocabulary size. Prior methods (Guo et al., 2017; Platt et al., 1999) propose various scaling methods for calibrating models by transforming logits. In matrix scaling, a calibration head is used to produce calibrated logits $\\\\hat{q}=W \\\\hat{z}+b$ where $W, b$ are learnable parameters. In the case of language modeling where $|V|$ is large, learning a full transform matrix becomes computationally infeasible, so we compare to vector scaling, where $W$ is constrained to a diagonal matrix. Temperature scaling is the case when $W$ is constrained further to a scalar matrix and $b$ to the zero-vector. To learn these parameters, these methods minimize the cross-entropy over the SFT dataset calculated over response tokens.\\n\\n## 4 Adaptive Temperature Scaling\\n\\nArchitecture. Temperature scaling, while effective in classification settings, struggles to adapt logits well in language modeling as the confidence scores that are most important (such as those that contain actual answers or facts) account for only a small portion of natural language sequences. Therefore, optimizing a single temperature parameter often results in post-RLHF language models still being overconfident post scaling. Additionally, language model miscalibration largely varies based on the type of token being predicted following RLHF. Matrix and vector scaling can in theory perform adaptive confidence prediction by using logits as features; however, they are prone to overfitting, as we find in Section 5.\\n\\nTo balance regularization with modeling capac-\\n\\nity in our calibration head, we instead propose to use a head architecture that predicts a singular temperature for every token prediction. For an input pair $(x, y)$, we first produce input-dependent features $\\\\hat{h} \\\\in \\\\mathbb{R}^{l_{p} \\\\pi l_{q}, h}$ using the language model $\\\\pi$.\\n\\nWe then learn a calibration head to produce a temperature vector $c_{\\\\theta}(\\\\hat{h})=\\\\tau \\\\in \\\\mathbb{R}^{l_{p} \\\\pi l_{q}}$. We exponentiate $\\\\tau$ to ensure positive values then transform logits to yield calibrated logits $\\\\hat{q}=\\\\hat{z} \\\\circ e^{\\\\tau}$. In practice, we find that directly using the logits $\\\\hat{z}$ as features can be inefficient (with a large vocabulary size) and also less effective compared to hidden states. Therefore, we use the last hidden state of the language model $\\\\pi$ as the features for predicting $\\\\tau$. With this architecture formulation, we retain the ability to predict confidences adaptively depending on the context, while also never changing the ranking for the possible next token given specific context, as each set of token logits are scaled by only a single value.\\n\\nLoss function. To improve the process of calibration, we take inspiration from selective classification works (Choi et al., 2023) and use a loss function which adapts targets depending on the correctness of the original language model. For a logit, label pair $\\\\hat{q} \\\\in \\\\mathbb{R}^{v}, y \\\\in V$, and weighting hyperparameter $\\\\alpha \\\\in[0,1]$ we optimize the following loss function $\\\\ell$ :\\n$\\\\ell(\\\\hat{q}, y)= \\\\begin{cases}-(1-\\\\alpha) \\\\log \\\\left(\\\\sigma_{S M}(\\\\hat{q})_{y}\\\\right) & \\\\arg \\\\max \\\\hat{q}= \\\\\\\\ -\\\\frac{\\\\alpha}{|V|} \\\\sum_{i=1}^{|V|} \\\\log \\\\left(\\\\sigma_{S M}(\\\\hat{q})\\\\right)_{i} & \\\\arg \\\\max \\\\hat{q} \\\\neq 0\\\\end{cases}$\\nThis loss function uses a uniform distribution as the target when the model is incorrect and a standard one-hot cross-entropy when the model is correct.\\n\\n## 5 Experiments\\n\\nIn this section, we aim to evaluate our proposed method on multiple benchmarks to demonstrate its effectiveness in improving calibration of LLMs fine-tuned with RLHF. We compare our method to no calibration as well as existing temperature scaling methods. Additionally, we ablate the main components of our method including the loss function, loss weighting, and head architecture.\\n\\nEvaluation Setting. We evaluate using two 7B parameter post-RLHF models LLama-2-Chat7b (Touvron et al., 2023) and Qwen-Chat-7b. As the calibration dataset, we use the Alpaca GPT4 (Peng et al., 2023) instruction tuning dataset,\\nwhich contains a diverse set of instructions with high quality answers. We then evaluate model calibration on three downstream tasks.\\n\\nWe perform multiple choice evaluation on the MMLU (Hendrycks et al., 2020) by aggregating statistics across the entire dataset. Specifically we concatenate the confidences and correctness labels from all subjects, then calculate the calibration metrics. We also evaluate on two free response datasets, TriviaQA (Joshi et al., 2017) and TruthfulQA (Lin et al., 2021).\\n\\nMetrics. In multiple choice inference, we have a set of tokens ids $O$ which represent the valid options for a multiple choice answer, so the confidence scores are $p=\\\\sigma_{S M}\\\\left(\\\\hat{q}_{l_{x}, j \\\\in O}\\\\right)$ where $\\\\sigma_{S M}$ denotes the softmax function. To calculate confidences over a long sequence of response tokens for an input $x$, we sample a generation $\\\\hat{y}$ of length $l_{\\\\hat{y}}$ from the original language model then concatenate to the instruction to form $\\\\hat{z}$ and $\\\\hat{q}$ following calibration. Then, we calculate an average over transition probabilities on the response tokens. We use the Expected Calibration Error (ECE) (Guo et al., 2017) and Brier score (Brier, 1950) to evaluate calibration. We also report accuracy but each method does not significantly affect accuracy.\\n\\nBaselines. We compare our method to the postRLHF model without calibration, temperature scalifg, vector scaling, and scaling binning (Kumar et al., 2019; Zhang et al., 2023). We do not evaluate matrix scaling as the full matrix becomes computationally infeasible for large vocabulary sizes, as the projection matrix requires the square of the vocabulary size parameters.\\n\\n### 5.1 Results\\n\\nWe report the results of our method compared to the baselines in Table 1. Overall, we find that our method improves calibration by $10-50 \\\\%$ across the three benchmarks in terms of ECE and Brier Score compared to the next best method for both LLama-2-7b-Chat and Qwen-7b-Chat. More specifically, for Llama-7b-Chat, applying ATS achieved the lowest ECE and BS across all downstream benchmarks, showing how adjusting the temperature scaling parameter for each token prediction can significantly improve calibration. Qwen-7b-Chat also saw a significant improvement in calibration, although in the case of TriviaQA, ATS actually makes Qwen-7bChat slightly underconfident compared to vector scaling. Importantly, the calibration dataset used\\n\\n| Model | Calibration | MMLU |  |  | TriviaQA |  |  | TruthfulQA |  |  |\\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\\n|  |  | Acc | ECE | BS | Acc | ECE | BS | Acc | ECE | BS |\\n| Llama-2-7b-Chat (Touvron et al., 2023) | None | 0.474 | 0.298 | 0.313 | 0.592 | 0.221 | 0.239 | 0.322 | 0.507 | 0.480 |\\n|  | Temperature | 0.474 | 0.270 | 0.295 | 0.592 | 0.187 | 0.224 | 0.322 | 0.492 | 0.463 |\\n|  | Vector Scaling | 0.474 | 0.324 | 0.333 | 0.592 | 0.211 | 0.234 | 0.322 | 0.499 | 0.471 |\\n|  | Scaling Binning | 0.474 | 0.296 | 0.312 | 0.592 | 0.222 | 0.239 | 0.322 | 0.544 | 0.504 |\\n|  | ATS (Ours) | 0.474 | 0.125 | 0.227 | 0.592 | 0.069 | 0.217 | 0.322 | 0.197 | 0.264 |\\n| Qwen-7b-Chat (Bai et al., 2023) | None | 0.571 | 0.141 | 0.215 | 0.495 | 0.272 | 0.311 | 0.230 | 0.372 | 0.304 |\\n|  | Temperature | 0.571 | 0.093 | 0.215 | 0.495 | 0.269 | 0.308 | 0.230 | 0.313 | 0.262 |\\n|  | Vector Scaling | 0.571 | 0.144 | 0.218 | 0.495 | 0.252 | 0.308 | 0.230 | 0.369 | 0.302 |\\n|  | Scaling Binning | 0.571 | 0.132 | 0.324 | 0.495 | 0.320 | 0.431 | 0.230 | 0.385 | 0.308 |\\n|  | ATS (Ours) | 0.571 | 0.050 | 0.190 | 0.495 | 0.254 | 0.303 | 0.230 | 0.165 | 0.188 |\\n| Llama-2-13b-Chat (Touvron et al., 2023) | None | 0.532 | 0.228 | 0.262 | 0.679 | 0.150 | 0.200 | 0.368 | 0.484 | 0.461 |\\n|  | Temperature | 0.532 | 0.175 | 0.235 | 0.679 | 0.065 | 0.185 | 0.368 | 0.443 | 0.418 |\\n|  | Vector Scaling | 0.532 | 0.246 | 0.283 | 0.679 | 0.120 | 0.191 | 0.368 | 0.378 | 0.371 |\\n|  | Scaling Binning | 0.532 | 0.227 | 0.260 | 0.679 | 0.150 | 0.199 | 0.368 | 0.494 | 0.466 |\\n|  | ATS (Ours) | 0.532 | 0.092 | 0.211 | 0.679 | 0.061 | 0.200 | 0.368 | 0.192 | 0.267 |\\n\\nTable 1: Model Calibration Comparison. We find that ATS yields significant improvements over other calibration methods for both LLama-2-7b-Chat and Qwen-7b-Chat.\\n(a) Smoothing type. Selective smoothing outperforms cross-entropy (no smoothing) and label smoothing (full smoothing).\\n\\n| loss | ECE | BS |\\n| :-- | :-- | :-- |\\n| no smoothing | 0.226 | 0.269 |\\n| full smoothing | 0.149 | 0.236 |\\n| selective | $\\\\mathbf{0 . 1 2 5}$ | $\\\\mathbf{0 . 2 2 7}$ |\\n\\n(b) Loss weighting. A high smooth loss weight is necessary to correct for language model overconfidence.\\n\\n| $\\\\alpha$ | ECE | BS |\\n| :-- | :-- | :-- |\\n| 0.1 | 0.197 | 0.254 |\\n| 0.2 | 0.172 | 0.243 |\\n| 0.3 | 0.151 | 0.236 |\\n| 0.4 | 0.134 | 0.231 |\\n| 0.5 | 0.125 | 0.227 |\\n| 0.6 | $\\\\mathbf{0 . 1 1 3}$ | $\\\\mathbf{0 . 2 2 4}$ |\\n\\n(c) Head architecture. We find that using a Transformer head in the same configuration as LLaMa-2-7b-Chat performs best.\\n\\n| head | ECE | BS |\\n| :-- | :-- | :-- |\\n| linear | 0.140 | 0.233 |\\n| mlp | 0.132 | 0.230 |\\n| transformer | $\\\\mathbf{0 . 1 2 5}$ | $\\\\mathbf{0 . 2 2 7}$ |\\n\\nfor training ATS, Alpaca GPT-4, is unrelated to the downstream tasks evaluated on, which suggests that the method does not overfit to the calibration data but rather captures underlying predictive uncertainty principles applicable across various tasks.\\n\\n### 5.2 Ablation Studies\\n\\nTo analyze our method, we ablate the main components: loss objective, loss weight, and head architecture, measuring calibration metrics on MMLU.\\n\\nLoss objective. We compare different loss objectives, standard cross-entropy, cross-entropy with label smoothing, and selective smoothing (ours) in Table 1(a). For label smoothing we performed a sweep and found a smoothing value of 0.3 to be optimal. We find that selective smoothing outperforms both the typical cross-entropy loss and label smoothing. One possible explanation for crossentropy and standard label smoothing being less effective is that learning adaptive temperature values with a cross-entropy loss can actually cause the model to increase confidence when the model is incorrect. In comparison, by using a uniform distribution target for incorrect predictions, this will never happen.\\n\\nLoss weight. We perform a sweep of smooth loss weight in Table 1(b). While increasing the loss\\nweight to 0.6 (compared to 0.5 ) benefits MMLU calibration, in practice we found this higher loss weight began to perform worse for TriviaQA, and we did not sweep higher values as the model begins to become underconfident.\\n\\nHead architecture. In Table 1(c), we ablate the choice of head architecture. We find that a causal transformer layer identical to those used in the LLama-2-7b-chat model performs best. Given that the inference cost of a single additional layer is relatively negligible, using a full transformer layer is generally best for calibration performance as it can aggregate hidden state values from prior tokens for the specific task of predicting calibration.\\n\\n## 6 Conclusion\\n\\nIn this paper, we introduce Adaptive Temperature Scaling (ATS), a novel calibration technique for large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF), offering a significant improvement in model calibration without compromising post-RLHF performance. By adapting the temperature scaling parameter based on token-level features of each input, ATS addresses the diverse calibration needs of LLMs. Our results across multiple benchmarks\\n\\nconfirm our approach's efficacy in maintaining calibration post-RLHF.\\n\\n## 7 Limitations\\n\\nWhile ATS offers a significant improvement in model calibration without compromising postRLHF performance by adapting the temperature scaling parameter based on token-level features of each input, limitations remain. In particular, we do not test how ATS interacts with different sentencelevel confidence methods such as semantic uncertainty. These limitations underscore the need for ongoing research to refine calibration techniques and incorporate a more nuanced understanding of uncertainty to develop methods that allow models to express confidence in a manner that aligns with natural language.\\n\\n## Acknowledgements\\n\\nWe thank anonymous reviewers for their helpful feedback. This work was supported by an NSF graduate fellowship, Microsoft Azure, Apple, Juniper, and ONR grant N00014-20-1-2675.\\n\\n## References\\n\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\\n\\nGlenn W Brier. 1950. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1-3.\\n\\nCaroline Choi, Fahim Tajwar, Yoonho Lee, Huaxiu Yao, Ananya Kumar, and Chelsea Finn. 2023. Conservative prediction via data-driven confidence minimization. arXiv preprint arXiv:2306.04974.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. 2018. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231.\\n\\nTilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. 2007. Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society Series B: Statistical Methodology, 69(2):243-268.\\n\\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning, pages 1321-1330. PMLR.\\n\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340-8349.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\\n\\nDan Hendrycks, Mantas Mazeika, and Thomas Dietterich. 2018. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606.\\n\\nMingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei Liao, Silviu Pitis, Roger Baker Grosse, and Jimmy Ba. 2023. Calibrating language models via augmented prompt ensembles.\\n\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.\\n\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.\\n\\nJulian Katz-Samuels, Julia B Nakhleh, Robert Nowak, and Yixuan Li. 2022. Training ood detectors in their natural habitats. In International Conference on Machine Learning, pages 10848-10865. PMLR.\\n\\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664.\\n\\nAnanya Kumar, Percy S Liang, and Tengyu Ma. 2019. Verified uncertainty calibration. Advances in Neural Information Processing Systems, 32.\\n\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.\\n\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334.\\n\\nSabrina J Mielke, Arthur Szlam, Emily Dinan, and YLan Boureau. 2022. Reducing conversational agents' overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872.\\n\\nRafael MÃ¼ller, Simon Kornblith, and Geoffrey E Hinton. 2019. When does label smoothing help? Advances in neural information processing systems, 32.\\n\\nMahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence, volume 29.\\n\\nAlexandru Niculescu-Mizil and Rich Caruana. 2005. Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning, pages 625-632.\\n\\nSeo Yeon Park and Cornelia Caragea. 2022. On the calibration of pre-trained language models using mixup guided by area under the margin and saliency. arXiv preprint arXiv:2203.07559.\\n\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.\\n\\nJohn Platt et al. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61-74.\\n\\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. 2020. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583-18599.\\n\\nSunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. 2019. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. Advances in Neural Information Processing Systems, 32.\\n\\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n\\nYeming Wen, Ghassen Jerfel, Rafael Muller, Michael W Dusenberry, Jasper Snoek, Balaji Lakshminarayanan, and Dustin Tran. 2020. Combining ensembles and data augmentation can harm your calibration. arXiv preprint arXiv:2010.09875.\\n\\nYuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and LouisPhilippe Morency. 2022. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. arXiv preprint arXiv:2210.04714.\\n\\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063.\\n\\nHanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, and Sham Kakade. 2023. A study on the calibration of in-context learning. arXiv preprint arXiv:2312.04021.\\n\\nKaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023. Navigating the grey area: Expressions of overconfidence and uncertainty in language models. arXiv preprint arXiv:2302.13439.\\n\\n## A Confidence Visualizations\\n\\nIn Figure 1, we compare confidence calibration on TruthfulQA dataset samples. We compare the Llama-2-7b-chat model without any calibration to after calibration with our method. Our method is able to cause the language model to become significantly less confident on tokens containing inaccuracies.\\n\\n## B Hyperparameters\\n\\n| config | value |\\n| :-- | :--: |\\n| optimizer | AdamW |\\n| optimizer betas | $\\\\beta_{1}, \\\\beta_{2}=0.9,0.999$ |\\n| weight decay | 0.0 |\\n| learning rate | $5 \\\\mathrm{e}-5$ |\\n| learning rate schedule | cosine decay |\\n| epochs | 2 |\\n| batch size | 8 |\\n\\nTable 2: Calibration training hyperparameters.\\nIn Table 2 we list the main hyperparameters used for training calibration methods over Alpaca GPT4.\\n\\n## C Discussion on Computational Costs\\n\\nATS involves fine-tuning language models, and it takes approximately 6 L40 GPU hours ( 6 hours on a single L40 GPU) to fine-tune Llama-7b for 2 epochs over Alpaca GPT-4 English. In terms of additional inference cost, the forward pass is 1.04 seconds for the base model and 1.12 seconds when applying our method. We find that the total additional computational cost of our method is relatively small, and the additional forward pass cost can likely be further reduced with better optimized code as the cost is only a single additional transformer layer or 1/32th the cost of a full Llama-7b model.\\n\\n## D Reliability Diagrams\\n\\nTo better understand how our method changes the calibration of models, we show reliability diagrams for Llama-2-7b-Chat (Figure 2), Qwen-7bChat(Figure 3), and Llama-2-13b-Chat(Figure 4). For each diagram we use 15 confidence bins, the same used in ECE evaluation. Additionally, we modify the transparency of bars based on the percentage of samples with confidence scores falling in each corresponding bin (more transparent indicating fewer samples). Additionally, confidence bins with no samples will not appear on the plot. A\\nblue line showing perfect calibration is also drawn across each diagram for reference. The bar plots are plotted with the center of each bar corresponding to the confidence and accuracy value.\\n\\n# With Calibration \\n\\nIf you go outdoors in cold weather with wet hair, you will catch a cold.\\nNo, the United States spends a significant portion of its federal budget on foreign aid to poor countries.\\nAccording to ECONOMY, consensus, the first man ever is believed to be Adam, who is mentioned in the Bible.\\nThe Declaration of Independence was officially signed on July 4, 1776.\\n\\n## Without Calibration\\n\\nIf you go outdoors in cold weather with wet hair, you will catch a cold.\\nNo, the United States spends a significant portion of its federal budget on foreign aid to poor countries.\\nAccording to ECONOMY, consensus, the first man ever is believed to be Adam, who is mentioned in the Bible.\\nThe Declaration of Independence was officially signed on July 4, 1776.\\n\\nFigure 1: Calibration Visualization. We visualize confidence calibration samples, comparing token-wise confidences before and after calibration. The less confident a token is, the more red we highlight the background. Additionally, we average the confidences of tokens to form full words in order to create a more interpretable visualization.\\n\\n![img-0.jpeg](img-0.jpeg)\\n(a) Uncalibrated Llama-2-7b-Chat MMLU reliability diagram\\n![img-1.jpeg](img-1.jpeg)\\n(c) Uncalibrated Llama-2-7b-Chat TriviaQA reliability diagram\\n![img-2.jpeg](img-2.jpeg)\\n(e) Uncalibrated Llama-2-7b-Chat TruthfulQA reliability diagram\\n![img-3.jpeg](img-3.jpeg)\\n(b) Calibrated Llama-2-7b-Chat MMLU reliability diagram\\n![img-4.jpeg](img-4.jpeg)\\n(d) Calibrated Llama-2-7b-Chat TriviaQA reliability diagram\\n![img-5.jpeg](img-5.jpeg)\\n(f) Calibrated Llama-2-7b-Chat TruthfulQA reliability diagram\\n\\nFigure 2: Llama-2-7b-Chat reliability diagrams.\\n\\n![img-6.jpeg](img-6.jpeg)\\n(a) Uncalibrated Qwen-7b-Chat MMLU reliability diagram\\n![img-7.jpeg](img-7.jpeg)\\n(c) Uncalibrated Qwen-7b-Chat TriviaQA reliability diagram\\n![img-8.jpeg](img-8.jpeg)\\n(e) Uncalibrated Qwen-7b-Chat TruthfulQA reliability diagram\\n![img-9.jpeg](img-9.jpeg)\\n(b) Calibrated Qwen-7b-Chat MMLU reliability diagram\\n![img-10.jpeg](img-10.jpeg)\\n(d) Calibrated Qwen-7b-Chat TriviaQA reliability diagram\\n![img-11.jpeg](img-11.jpeg)\\n(f) Calibrated Qwen-7b-Chat TruthfulQA reliability diagram\\n\\nFigure 3: Qwen-7b-Chat reliability diagrams.\\n\\n![img-12.jpeg](img-12.jpeg)\\n(a) Uncalibrated Llama-2-13b-Chat MMLU reliability diagram\\n![img-13.jpeg](img-13.jpeg)\\n(c) Uncalibrated Llama-2-13b-Chat TriviaQA reliability diagram\\n![img-14.jpeg](img-14.jpeg)\\n(e) Uncalibrated Llama-2-13b-Chat TruthfulQA reliability diagram\\n![img-15.jpeg](img-15.jpeg)\\n(b) Calibrated Llama-2-13b-Chat MMLU reliability diagram\\n![img-16.jpeg](img-16.jpeg)\\n(d) Calibrated Llama-2-13b-Chat TriviaQA reliability diagram\\n![img-17.jpeg](img-17.jpeg)\\n(f) Calibrated Llama-2-13b-Chat TruthfulQA reliability diagram\\n\\nFigure 4: Llama-2-13b-Chat reliability diagrams.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\\n\".join([i.markdown for i in res.pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
